{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a257207b",
   "metadata": {},
   "source": [
    "# Día 1 — Conceptos básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f0092-6bcb-4dae-9044-029d78324901",
   "metadata": {},
   "source": [
    "````{admonition} Resumen\n",
    ":class: tip\n",
    "\n",
    "**Objetivo del día:** Comprender la naturaleza matricial de las imágenes, los espacios de color más usados (BGR/RGB/GRAY) y dominar las operaciones básicas E/S (imágenes y vídeo) y conversión de color en OpenCV. Dominar la estructura de una imagen como `numpy.ndarray` (análoga a `cv::Mat`), y asentar bases matemáticas mínimas: tensores y color lineal vs. no lineal.\n",
    "\n",
    "**Requisitos:** Python ≥ 3.9, `opencv-python`, `numpy`, `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4997ed",
   "metadata": {},
   "source": [
    "\n",
    "## Índice\n",
    "1. Ecosistema y conceptos clave\n",
    "2. `cv::Mat` vs `numpy.ndarray`: dtype, shape, strides, memoria\n",
    "3. Canales de color: BGR/RGB/GRAY y conversión\n",
    "4. ROI, copia vs. vista (copy vs. view)\n",
    "5. E/S de imágenes: `imread`, `imwrite`\n",
    "6. Vídeo con `VideoCapture`\n",
    "7. Cronometría: copias vs. vistas\n",
    "8. **Stretch**: `safe_imread` robusto\n",
    "9. Ejercicios (con soluciones ocultables)\n",
    "10. Lecturas y referencias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac2f1f",
   "metadata": {},
   "source": [
    "## 1. Conceptos clave y fundamentos matemáticos\n",
    "\n",
    "### 1.1 Imagen como función/matriz\n",
    "Una imagen digital puede modelarse como una función **muestrada y cuantizada**:\n",
    "\n",
    "$$\n",
    "I:\\Omega \\subset \\mathbb{Z}^2 \\to \\mathbb{R}^C,\\quad I(x,y) = \\big(I_1(x,y),\\dots,I_C(x,y)\\big)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $(x,y)$ son coordenadas de píxel en una retícula regular,\n",
    "- $C$ es el número de canales (1 en escala de grises, 3 en color),\n",
    "- la intensidad suele estar cuantizada en 8 bits `uint8`$(0$–$255)$ o en punto flotante normalizado `float32` $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de04f7f-ef03-429d-800b-da2678419a2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.2 Imágenes como tensores\n",
    "\n",
    "Desde un punto de vista matemático, una imagen digital puede entenderse como un **tensor**: una estructura multidimensional que contiene valores numéricos organizados en ejes.  \n",
    "\n",
    "- Una imagen en **escala de grises** se representa como un tensor de orden 2:  $I \\in \\mathbb{R}^{H \\times W}$ donde $(H)$ es la altura (número de filas) y $(W)$ el ancho (número de columnas).  Cada elemento $I_{ij}$ representa la intensidad luminosa del píxel en la fila $i$ y columna $j$.\n",
    "\n",
    "- Una imagen **en color**, en cambio, se representa como un tensor de orden 3: $I \\in \\mathbb{R}^{H \\times W \\times C}$ donde $C = 3$ corresponde a los tres canales de color (por ejemplo, **BGR** en OpenCV o **RGB** en Matplotlib).\n",
    "\n",
    "Cada canal codifica una **componente espectral** de la luz. Así, un píxel completo puede expresarse como un vector tridimensional:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{ij} = \n",
    "\\begin{bmatrix}\n",
    "B_{ij} \\\\ \n",
    "G_{ij} \\\\ \n",
    "R_{ij}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "en el espacio de color BGR.  \n",
    "\n",
    "En operaciones matriciales, estas imágenes se tratan como tensores en los que las operaciones (suma, resta, multiplicación escalar, etc.) se aplican **por elemento**. `NumPy` facilita estas operaciones mediante el **broadcasting**, que ajusta automáticamente las dimensiones compatibles.\n",
    "\n",
    "```{tip}\n",
    "El broadcasting en `NumPy` es un mecanismo inteligente que permite realizar operaciones aritméticas entre arrays de diferentes dimensiones de forma automática y eficiente, sin necesidad de duplicar físicamente los datos en memoria. Este sistema funciona mediante reglas bien definidas que comparan las dimensiones de los arrays desde la derecha hacia la izquierda, determinando su compatibilidad cuando las dimensiones son iguales, cuando una de ellas es 1, o cuando una dimensión no existe en uno de los arrays. En esencia, el broadcasting \"estira\" virtualmente los arrays más pequeños para que coincidan con la forma del array más grande, permitiendo operaciones elemento por elemento que de otra manera requerirían costosos bucles o replicaciones explícitas de datos.\n",
    "\n",
    "Esta funcionalidad no solo hace que el código sea más conciso y legible, sino que también mejora significativamente el rendimiento al vectorizar las operaciones internamente. Por ejemplo, cuando sumamos un escalar a un array multidimensional, NumPy automáticamente propaga ese valor único a través de todos los elementos del array, o cuando operamos entre un vector fila y una matriz, el sistema replica inteligentemente el vector a lo largo de las dimensiones necesarias. Sin embargo, el broadcasting tiene sus límites y falla cuando las dimensiones de los arrays son fundamentalmente incompatibles según las reglas establecidas, protegiendo así al usuario de resultados erróneos. Esta característica constituye una piedra angular en aplicaciones científicas y de machine learning, donde las operaciones entre datasets de distintas formas son frecuentes y la eficiencia computacional es crucial.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d73230-2fd8-43b6-9c08-c23e471b73dc",
   "metadata": {},
   "source": [
    "## 2. Espacios de color: concepto fundamental\n",
    "\n",
    "### 2.1 Conceptos fundamentales\n",
    "\n",
    "Un **espacio de color** es una forma de representar los colores posibles dentro de un modelo matemático determinado. Los más comunes son:\n",
    "\n",
    "- **RGB / BGR**: El espacio de color RGB (Red, Green, Blue) es un modelo de color aditivo fundamental en la representación digital de imágenes, basado en la combinación de tres componentes luminosos primarios: rojo, verde y azul. RGB opera mediante la síntesis aditiva de luz, donde diferentes intensidades de estos tres canales se mezclan para producir la gama completa de colores visibles, desde el negro absoluto (cuando todos los valores son cero) hasta el blanco puro (cuando los tres componentes alcanzan su máxima intensidad). La estructura de RGB lo hace extremadamente eficiente para el procesamiento computacional y el almacenamiento de datos de imagen, aunque resulta menos intuitivo cuando se intentan realizar ajustes cromáticos específicos, ya que modificaciones aparentemente simples como cambiar el tono de un color requieren alteraciones coordinadas en los tres canales simultáneamente. `BGR` es el espacio de color por defecto de OpenCV. Mantiene la misma base funcional aunque el orden de los tres canales es azul (B), verde (G) y rojo (R).\n",
    "  \n",
    "- **GRAY**: También conocido como escala de grises, es un modelo de representación cromática que reduce toda la información de color a un único canal de intensidad lumínica, eliminando por completo los componentes de tonalidad y saturación. En este sistema unidimensional, cada píxel se representa mediante un valor numérico que exclusivamente codifica su nivel de brillo, oscilando típicamente entre el negro absoluto (valor 0) y el blanco puro (valor máximo, usualmente 255 en imágenes de 8 bits), con toda la gama de grises intermedios distribuidos proporcionalmente según su luminosidad perceptual. Esta transformación se calcula generalmente mediante una fórmula ponderada que combina los canales originales RGB:\n",
    "$\n",
    "0.299 \\cdot R + 0.587 \\cdot G + 0.114 \\cdot B\n",
    "$\n",
    "donde los coeficientes reflejan la sensibilidad diferencial del ojo humano a cada color primario, priorizando el verde sobre el rojo y el azul para lograr una conversión perceptualmente precisa. Al descartar toda la información cromática y conservar únicamente los datos de luminancia, el espacio GRAY resulta especialmente útil para aplicaciones de procesamiento de imágenes que se centran en análisis estructural, detección de bordes, reconocimiento de patrones o cualquier tarea donde la información de textura y contraste sea más relevante que el color mismo, además de ofrecer una significativa reducción en la complejidad computacional y los requisitos de almacenamiento al trabajar con un solo canal en lugar de tres.\n",
    "    \n",
    "- **HSV**: El espacio de color HSV (Hue, Saturation, Value) es un modelo de representación cromática que organiza los colores de forma más intuitiva que el tradicional RGB, basándose en cómo los seres humanos perciben y describen los atributos del color. En este sistema tridimensional, el **Matiz (Hue)** se representa como un círculo cromático continuo donde cada ángulo corresponde a un color puro específico, desde el rojo en 0° hasta los violetas en 360°. La **Saturación (Saturation)** controla la intensidad o pureza del color, variando desde tonos grisáceos neutros en el valor mínimo hasta colores completamente vivos y puros en el máximo. El **Valor (Value)** determina la luminosidad general, escalando desde el negro absoluto hasta el máximo brillo del color, independientemente de su tono. Esta separación entre el aspecto cromático (matiz), la intensidad del color (saturación) y el brillo (valor) permite manipulaciones más naturales en aplicaciones de edición gráfica, selección de paletas de colores y procesamiento de imágenes, ya que los usuarios pueden ajustar cada componente perceptual por separado sin afectar necesariamente a los demás, facilitando operaciones como cambiar tonos específicos, ajustar contraste o crear degradados armónicos.\n",
    "\n",
    "- **LAB**: El espacio de color LAB, también conocido como CIELAB, es un modelo de color diseñado para ser perceptualmente uniforme, lo que significa que las distancias matemáticas entre colores en este espacio se correlacionan directamente con las diferencias percibidas por el ojo humano. Organizado en tres ejes, la `L` representa la luminosidad* (desde el negro 0 hasta el blanco 100), el eje `A` define la posición entre el verde (valores negativos) y el rojo (valores positivos), y el eje `B` representa la posición entre el azul (valores negativos) y el amarillo (valores positivos). A diferencia de los espacios dependientes de dispositivos como RGB, LAB es un espacio de color independiente y pretende encapsular todo el rango de colores que los humanos pueden percibir. Esta característica lo hace particularmente valioso en aplicaciones de edición de imágenes avanzada, control de calidad de color en la industria, y algoritmos de visión por computadora donde la percepción visual precisa y la consistencia del color bajo diferentes condiciones de iluminación son fundamentales, ya que permite ajustes de color que resultan más intuitivos y visualmente coherentes.\n",
    "\n",
    "- **YCrCb**:  Este espacio de color es un modelo de representación cromática que separa la información de una imagen en tres componentes distintos: `Y` representa la luminancia (componente de brillo), `Cb` codifica la diferencia entre el componente azul y la luminancia, y `Cr` representa la diferencia entre el componente rojo y la luminancia. Este sistema se diseñó específicamente para sistemas de video y compresión de imágenes digitales, aprovechando la menor sensibilidad del ojo humano a las variaciones cromáticas en comparación con las variaciones de brillo. Al separar la información de luminancia de la crominancia, YCbCr permite aplicar técnicas de submuestreo (como 4:2:2 o 4:2:0) donde la información de color se almacena con menor resolución que la información de brillo sin pérdidas perceptibles significativas en la calidad visual. Esta característica lo hace fundamental en estándares de compresión ampliamente utilizados como JPEG para imágenes estáticas y MPEG/H.264 para video digital, donde se logran altas tasas de compresión al reducir selectivamente los datos cromáticos mientras se preserva la calidad en los detalles estructurales capturados por el componente de luminancia. \n",
    "\n",
    "OpenCV gestiona internamente las imágenes en el espacio de color BGR (Blue-Green-Red) debido a condicionantes históricos vinculados a las primeras implementaciones de bibliotecas de visión artificial y a convenciones heredadas de los primeros sistemas de captura. Esta particularidad genera una discrepancia fundamental con la mayoría de ecosistemas de visualización (como `Matplotlib` o `PIL/Pillow`) que operan bajo el estándar RGB (Red-Green-Blue), estructura alineada con los modelos perceptuales humanos y los espacios colorimétricos convencionales.\n",
    "\n",
    "Para resolver esta disonancia estructural, OpenCV incorpora mecanismos de transformación bidireccional mediante funciones especializadas en conversión de espacios de color `cv2.Color()` , permitiendo una transposición eficiente y sin pérdida de información entre ambos modelos mediante operaciones de reordenamiento canales. Esta capa de abstracción garantiza la interoperabilidad entre subsistemas de procesamiento y visualización, simplificando flujos de trabajo en *pipelines* de procesamiento de imágenes donde la coherencia en la representación cromática es crucial para el análisis cuantitativo y la interpretación visual de resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911033e8-c9ed-4abf-8855-ac493e086f8c",
   "metadata": {},
   "source": [
    "### 2.2 Modelos matemáticos simplificados\n",
    "\n",
    "- **HSV (normalizado):**\n",
    "  Sea $(R,G,B)\\in[0,1]^3$, $M=\\max(R,G,B)$, $m=\\min(R,G,B)$, $\\Delta=M-m$.\n",
    "\n",
    "  \n",
    "  $$\n",
    "  V=M,\\quad S=\\begin{cases}\n",
    "  0 & \\text{si } M=0,\\\\\n",
    "  \\Delta/M & \\text{en otro caso}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "  \n",
    "  El **tono** $H$ (en grados) es:\n",
    "\n",
    "  ```{math}\n",
    "  H = \\begin{cases}\n",
    "  0 & \\Delta=0,\\\\\n",
    "  60^\\circ \\cdot \\big((G-B)/\\Delta \\bmod 6\\big) & M=R,\\\\\n",
    "  60^\\circ \\cdot \\big((B-R)/\\Delta + 2\\big) & M=G,\\\\\n",
    "  60^\\circ \\cdot \\big((R-G)/\\Delta + 4\\big) & M=B.\n",
    "  \\end{cases}\n",
    "  ```\n",
    "  \n",
    "- **YCrCb (BT.601):**\n",
    "\n",
    "  ```{math}\n",
    "  Y  = 0.299R + 0.587G + 0.114B,\\quad\n",
    "  Cb = \\frac{B - Y}{1.772},\\quad\n",
    "  Cr = \\frac{R - Y}{1.402}.\n",
    "  ```\n",
    "  ```{tip}\n",
    "  Los coeficientes estándares BT.601 y BT.709 son especificaciones técnicas desarrolladas por la Unión Internacional de Telecomunicaciones (ITU) que definen los parámetros para la conversión entre espacios de color en sistemas de video y televisión digital.\n",
    "\n",
    "  BT.601 (Recomendación ITU-R BT.601), también conocido como \"Rec. 601\", fue establecido originalmente para la televisión digital estándar (SDTV) y se utiliza principalmente para contenido de definición estándar con resoluciones como 720x480 (NTSC) y 720x576 (PAL). Sus coeficientes para la conversión a YCbCr reflejan las características de los CRT tradicionales y emplean las siguientes ponderaciones: Y = 0.299R + 0.587G + 0.114B.\n",
    "\n",
    "  BT.709 (Recomendación ITU-R BT.709), conocido como \"Rec. 709\", fue desarrollado para televisión de alta definición (HDTV) y constituye el espacio de color estándar para contenido HD y Full HD. Este estándar utiliza coeficientes diferentes: Y = 0.2126R + 0.7152G + 0.0722B, que están optimizados para las tecnologías de pantalla moderna como LCD, plasma y OLED, reflejando una mayor sensibilidad al verde acorde con la percepción visual humana contemporánea y las capacidades de los dispositivos de visualización actuales.\n",
    "\n",
    "  La diferencia fundamental entre ambos radica en que BT.709 asigna más peso al componente verde, reconociendo su mayor importancia en la percepción de luminancia por el ojo humano en sistemas de alta definición, mientras que BT.601 mantiene una distribución más equilibrada entre rojo y verde para los sistemas de definición estándar. En OpenCV, la conversión utiliza por defecto los coeficientes BT.601, pero puede especificarse el estándar deseado mediante parámetros adicionales en las funciones de procesamiento de video.\n",
    "  ```\n",
    "  \n",
    "- **LAB** se define una serie de pasos intermedios que comprenden: RGB→CIE XYZ (lineal), normalización con iluminante de referencia, transformación no lineal XYZ→LAB (no lineal).\n",
    "\n",
    "  - Conversión RGB → CIE XYZ\n",
    " \n",
    "    ```{math}\n",
    "    \\begin{aligned}\n",
    "    X &= 0.412453 \\cdot R + 0.357580 \\cdot G + 0.180423 \\cdot B \\\\\n",
    "    Y &= 0.212671 \\cdot R + 0.715160 \\cdot G + 0.072169 \\cdot B \\\\\n",
    "    Z &= 0.019334 \\cdot R + 0.119193 \\cdot G + 0.950227 \\cdot B\n",
    "    \\end{aligned}\n",
    "    ```\n",
    "\n",
    "  - Normalización con Iluminante de Referencia (D65)\n",
    " \n",
    "    ```{math}\n",
    "    \\begin{aligned}\n",
    "    X_n &= 0.950456 \\\\\n",
    "    Y_n &= 1.000000 \\\\\n",
    "    Z_n &= 1.088754 \\\\\n",
    "    \\\\\n",
    "    X' &= \\frac{X}{X_n} \\\\\n",
    "    Y' &= \\frac{Y}{Y_n} \\\\\n",
    "    Z' &= \\frac{Z}{Z_n}\n",
    "    \\end{aligned}\n",
    "    ``` \n",
    "\n",
    "\n",
    "  - Función de transformación no lineal\n",
    " \n",
    "    ```{math}\n",
    "    f(t) = \n",
    "    \\begin{cases}\n",
    "    t^{1/3} & \\text{si } t > 0.008856 \\\\\n",
    "    7.787 \\cdot t + \\frac{16}{116} & \\text{si } t \\leq 0.008856\n",
    "    \\end{cases}\n",
    "    ```\n",
    "\n",
    "  - Coordenadas LAB\n",
    " \n",
    "    ```{math}\n",
    "    \\begin{aligned}\n",
    "    L^* &= 116 \\cdot f(Y') - 16 \\\\\n",
    "    a^* &= 500 \\cdot \\left[f(X') - f(Y')\\right] \\\\\n",
    "    b^* &= 200 \\cdot \\left[f(Y') - f(Z')\\right]\n",
    "    \\end{aligned}\n",
    "    ``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f2f4d-a767-48a3-be72-c2d2ccd3c654",
   "metadata": {},
   "source": [
    "### 2.3. Espacio de color lineal vs. no lineal (gamma)\n",
    "\n",
    "Las cámaras y pantallas **no capturan ni muestran la luz de forma lineal** con respecto a la energía luminosa.  \n",
    "\n",
    "Por motivos fisiológicos y técnicos, la mayoría de las imágenes digitales usan el espacio `sRGB`, que aplica una **corrección gamma**: una función no lineal que comprime los valores de luminancia.\n",
    "\n",
    "- En un espacio **lineal**, la intensidad $L$ es **proporcional** a la energía luminosa real.\n",
    "- En un espacio **no lineal (sRGB)**, el valor almacenado $V$ está relacionado con $L$ por una potencia gamma:\n",
    "  $$\n",
    "  V = L^{1/\\gamma}, \\quad  \\gamma \\approx 2.2\n",
    "  $$\n",
    "  \n",
    "  y su inversa:\n",
    "  $$\n",
    "  L = V^{\\gamma}\n",
    "  $$\n",
    "\n",
    "Por tanto, para realizar cálculos físicos o matemáticos (por ejemplo, combinaciones de imágenes, promedios, o ajustes de exposición) debemos **linealizar** la imagen:\n",
    "  \n",
    "1. Normalizar los valores de 8 bits a rango $[0, 1]$:\n",
    "   $$\n",
    "   V' = \\frac{V}{255}\n",
    "   $$\n",
    "2. Aplicar la corrección inversa:\n",
    "   $$\n",
    "   L = (V')^{\\gamma}\n",
    "   $$\n",
    "\n",
    "Y una vez terminados los cálculos, para visualizar correctamente en pantalla debemos **reaplicar la gamma directa**:\n",
    "$$\n",
    "V' = L^{1/\\gamma}, \\quad V = 255 \\cdot V'\n",
    "$$\n",
    "\n",
    "````{warning} Ejemplo\n",
    "Supongamos dos píxeles con valores de luminancia física $0.25$ y $0.50$.  \n",
    "\n",
    "En el espacio **lineal**, el segundo es exactamente el doble de brillante.  Sin embargo, en sRGB (no lineal):\n",
    "\n",
    "$$\n",
    "V_1 = (0.25)^{1/2.2} \\approx 0.53,\\quad\n",
    "V_2 = (0.50)^{1/2.2} \\approx 0.73\n",
    "$$\n",
    "\n",
    "Al convertir de nuevo a 8 bits, los valores son 135 y 186, y **la diferencia ya no es lineal**: el ojo humano percibe el segundo píxel menos de “el doble” de brillante, ajustando la percepción a la realidad visual.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aace3f-a7a9-47ea-9012-0e7cadf9091a",
   "metadata": {},
   "source": [
    "```{admonition} Conclusión práctica\n",
    "\n",
    "- Para **visualizar imágenes**, se usa sRGB (no lineal).  \n",
    "- Para **procesarlas matemáticamente** (promedios, convoluciones, blending, etc.), conviene transformarlas a un **espacio lineal** y al finalizar re-aplicar la gamma inversa para visualizar.\n",
    "- OpenCV no aplica corrección gamma de forma automática: los valores que devuelve `cv2.imread()` están en el espacio sRGB estándar (no lineal).  \n",
    "\n",
    "En resumen, comprender los espacios de color y la diferencia entre representaciones lineales y no lineales es esencial para evitar errores de interpretación en la intensidad, el brillo o las operaciones matemáticas sobre imágenes.\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66388ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajustes de visualización en notebook\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "\n",
    "print(cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19192874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('img/synthetic.jpg')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Generamos una imagen sintética de prueba (gradiente + círculo) y la guardamos\n",
    "synthetic = np.zeros((240, 320, 3), dtype=np.uint8)\n",
    "for y in range(synthetic.shape[0]):\n",
    "    synthetic[y, :, 1] = np.uint8(255 * y / synthetic.shape[0])  # canal G con gradiente vertical\n",
    "cv2.circle(synthetic, center=(160,120), radius=60, color=(255, 0, 0), thickness=-1)  # BGR: azul lleno\n",
    "\n",
    "out_dir = Path('./img')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "synthetic_path = out_dir / 'synthetic.jpg'\n",
    "cv2.imwrite(str(synthetic_path), synthetic)\n",
    "synthetic_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6306ed",
   "metadata": {},
   "source": [
    "\n",
    "## 3. E/S de imágenes: `imread`, `imwrite` y comprobaciones básicas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652de96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RUTA DE ENTRADA: cambia 'synthetic_path' por tu imagen si lo deseas\n",
    "img_path = str(synthetic_path)  # por defecto usamos la imagen sintética\n",
    "img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"No se pudo leer la imagen en {img_path}. Comprueba la ruta y permisos.\")\n",
    "\n",
    "print('dtype:', img.dtype)\n",
    "print('shape (H, W, C):', img.shape)\n",
    "print('strides (bytes):', img.strides)  # NumPy muestra strides en bytes\n",
    "\n",
    "# Guardamos una copia\n",
    "saved_path = str(out_dir / 'copy_saved.png')\n",
    "cv2.imwrite(saved_path, img)\n",
    "print('Guardada copia en:', saved_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcea600",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Visualización: BGR vs. RGB en la **misma celda**\n",
    "OpenCV usa **BGR**, mientras que Matplotlib espera **RGB**. Debemos convertir antes de mostrar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "# Izquierda: mostrar el array en BGR (incorrecto en Matplotlib, pero lo forzamos para ilustrar la diferencia)\n",
    "axes[0].imshow(img)  # interpretado como RGB => colores \"raros\" porque el array está en BGR\n",
    "axes[0].set_title('Interpretado como RGB (input BGR)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Derecha: conversión BGR->RGB correcta\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "axes[1].imshow(img_rgb)\n",
    "axes[1].set_title('Convertido a RGB (correcto)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad72ed1",
   "metadata": {},
   "source": [
    "\n",
    "### Conversión a GRAY y verificación de canales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "print('GRAY dtype:', gray.dtype, 'shape:', gray.shape)  # H, W\n",
    "plt.imshow(gray, cmap='gray'); plt.title('GRAY'); plt.axis('off'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00fb3eb",
   "metadata": {},
   "source": [
    "\n",
    "## 5. ROI, **vista** vs. **copia**\n",
    "Una **ROI** con slicing NumPy suele ser **vista** (comparte memoria). Si modificas la vista, se modifica el original. Usa `.copy()` si quieres aislarla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab296e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h, w = img.shape[:2]\n",
    "roi = img[h//4: 3*h//4, w//4: 3*w//4]       # ROI central (vista)\n",
    "roi_copy = roi.copy()                       # copia independiente\n",
    "\n",
    "# Modificamos la vista: dibujamos un rectángulo rojo (BGR: (0,0,255)) en la ROI\n",
    "cv2.rectangle(roi, (5,5), (roi.shape[1]-6, roi.shape[0]-6), (0,0,255), 2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
    "axes[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)); axes[0].set_title('Imagen (modificada vía ROI)'); axes[0].axis('off')\n",
    "axes[1].imshow(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)); axes[1].set_title('ROI (vista)'); axes[1].axis('off')\n",
    "axes[2].imshow(cv2.cvtColor(roi_copy, cv2.COLOR_BGR2RGB)); axes[2].set_title('ROI copy (aislada)'); axes[2].axis('off')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d4948",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Strides: cómo se recorre la memoria\n",
    "Las **strides** son el número de **bytes** que hay que saltar para avanzar una unidad en cada eje. En una imagen `H×W×C` `uint8`, típicamente:\n",
    "- `stride_H = W*C` bytes para saltar a la siguiente fila,\n",
    "- `stride_W = C` bytes para pasar al siguiente pixel en la fila,\n",
    "- `stride_C = 1` byte para pasar al siguiente canal del mismo pixel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Strides img:\", img.strides, \" (bytes)\")\n",
    "print(\"dtype itemsize:\", img.dtype.itemsize, \"byte(s)\")\n",
    "print(\"Comprobación W*C:\", img.shape[1]*img.shape[2], \"vs stride fila en bytes:\", img.strides[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f39cd",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Vídeo con `cv2.VideoCapture`\n",
    "Abrimos un archivo o webcam. En entornos sin cámara, este bloque simplemente informará y continuará.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1523bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Intenta abrir webcam; si falla, informa. En tu entorno, cambia por un archivo de vídeo si lo deseas.\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"No se pudo abrir la cámara. Prueba con un archivo de vídeo en su lugar.\")\n",
    "else:\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if ret:\n",
    "        print(\"Frame leído:\", frame.shape, frame.dtype)\n",
    "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)); plt.title('Primer frame'); plt.axis('off'); plt.show()\n",
    "    else:\n",
    "        print(\"No se pudo leer un frame de la cámara.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78aa363",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Cronometría: copia vs. vista\n",
    "Medimos tiempo de **copiar** (`.copy()`) frente a obtener una **vista** (slicing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852c09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import timeit\n",
    "\n",
    "setup = \"import numpy as np; import cv2; img=np.random.randint(0,256,(1080,1920,3),dtype=np.uint8)\"\n",
    "time_copy = timeit.timeit(\"roi=img[100:900,200:1700].copy()\", setup=setup, number=50)\n",
    "time_view = timeit.timeit(\"roi=img[100:900,200:1700]\", setup=setup, number=50)\n",
    "print(f\"Copia: {time_copy:.6f} s (50 iters)\")\n",
    "print(f\"Vista: {time_view:.6f} s (50 iters)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d782ae40",
   "metadata": {},
   "source": [
    "\n",
    "## 9. **Stretch**: `safe_imread` (cargas con validación y errores con causa)\n",
    "Implementamos una función robusta para leer imágenes que:\n",
    "- Valida la ruta y la existencia del archivo,\n",
    "- Intenta leer en modo color/gray según parámetro,\n",
    "- Devuelve `(img, None)` si todo va bien; o `(None, error_message)` si algo falla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional, Tuple, Literal\n",
    "\n",
    "def safe_imread(path: str, mode: Literal['color','gray']='color') -> Tuple[Optional[np.ndarray], Optional[str]]:\n",
    "    \"\"\"Lee una imagen con validación de ruta y errores con causa.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Ruta al archivo de imagen.\n",
    "    mode : {'color','gray'}\n",
    "        Modo de lectura. 'color' => BGR, 'gray' => escala de grises.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (img, err) : Tuple[Optional[np.ndarray], Optional[str]]\n",
    "        img: np.ndarray si ok; None si falla.\n",
    "        err: None si ok; string con causa si falla.\n",
    "    \"\"\"\n",
    "    if not isinstance(path, (str, Path)):\n",
    "        return None, \"El parámetro 'path' debe ser str o Path.\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return None, f\"El archivo no existe: {p}\"\n",
    "    if not p.is_file():\n",
    "        return None, f\"La ruta no es un archivo: {p}\"\n",
    "    \n",
    "    flag = cv2.IMREAD_COLOR if mode == 'color' else cv2.IMREAD_GRAYSCALE\n",
    "    img = cv2.imread(str(p), flag)\n",
    "    if img is None:\n",
    "        return None, \"cv2.imread devolvió None. Puede ser un formato no soportado o un archivo corrupto.\"\n",
    "    return img, None\n",
    "\n",
    "# Prueba con nuestra imagen sintética\n",
    "ok_img, err = safe_imread(synthetic_path, mode='color')\n",
    "print(\"OK:\", ok_img is not None, \"| Error:\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c9c8e",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Ejercicios (con soluciones ocultables)\n",
    "\n",
    "**Ejercicio 1.** Carga una imagen desde disco, imprime su `dtype`, `shape` y `strides`.  \n",
    "**Ejercicio 2.** Muestra la misma imagen en BGR y en RGB en una **misma salida** con subplots.  \n",
    "**Ejercicio 3.** Extrae una ROI central como **vista**, dibuja un rectángulo y verifica que se modifica la imagen original. Repite con `.copy()` y verifica que **no** afecta al original.  \n",
    "**Ejercicio 4.** Convierte tu imagen a GRAY y guarda el resultado en PNG.  \n",
    "**Ejercicio 5.** Mide el tiempo (con `timeit`) de crear una vista vs. una copia para una ROI grande.\n",
    "\n",
    "<details>\n",
    "<summary><b>Solución (mostrar/ocultar)</b></summary>\n",
    "\n",
    "```python\n",
    "# Ejercicio 1\n",
    "img2, err = safe_imread(synthetic_path)\n",
    "assert err is None\n",
    "print(img2.dtype, img2.shape, img2.strides)\n",
    "\n",
    "# Ejercicio 2\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "axes[0].imshow(img2)  # BGR mal interpretado como RGB\n",
    "axes[0].set_title('Interpretado como RGB (input BGR)'); axes[0].axis('off')\n",
    "axes[1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title('Convertido a RGB'); axes[1].axis('off')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Ejercicio 3\n",
    "h, w = img2.shape[:2]\n",
    "roi_v = img2[h//4:3*h//4, w//4:3*w//4]  # vista\n",
    "cv2.rectangle(roi_v, (5,5), (roi_v.shape[1]-6, roi_v.shape[0]-6), (0,255,0), 2)\n",
    "plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)); plt.title('Original modificado'); plt.axis('off'); plt.show()\n",
    "\n",
    "img2b = img2.copy()\n",
    "roi_c = img2b[h//4:3*h//4, w//4:3*w//4].copy()  # copia\n",
    "cv2.circle(roi_c, (roi_c.shape[1]//2, roi_c.shape[0]//2), 20, (255,0,255), 2)\n",
    "# No afecta a img2b\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].imshow(cv2.cvtColor(img2b, cv2.COLOR_BGR2RGB)); ax[0].set_title('Original (intacto)'); ax[0].axis('off')\n",
    "ax[1].imshow(cv2.cvtColor(roi_c, cv2.COLOR_BGR2RGB)); ax[1].set_title('ROI copia (modificada)'); ax[1].axis('off')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Ejercicio 4\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "out_gray = str(out_dir / 'resultado_gray.png')\n",
    "cv2.imwrite(out_gray, gray2)\n",
    "\n",
    "# Ejercicio 5\n",
    "import timeit\n",
    "setup = \"import numpy as np; img=np.random.randint(0,256,(1080,1920,3),dtype=np.uint8)\"\n",
    "print('copy:', timeit.timeit(\"roi=img[100:900,200:1700].copy()\", setup=setup, number=50))\n",
    "print('view:', timeit.timeit(\"roi=img[100:900,200:1700]\", setup=setup, number=50))\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb99b9",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Lecturas y referencias\n",
    "\n",
    "- Documentación oficial de OpenCV (Python): https://docs.opencv.org/\n",
    "- Tutoriales (OpenCV-Python Tutorials): https://docs.opencv.org/master/d6/d00/tutorial_py_root.html\n",
    "- NumPy: https://numpy.org/doc/stable/\n",
    "- sRGB vs. linear (curva gamma): especificación sRGB IEC 61966-2-1; explicación práctica: https://entropymine.com/imageworsener/gamma/\n",
    "- Matplotlib (para visualización): https://matplotlib.org/stable/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5444d",
   "metadata": {},
   "source": [
    "\n",
    "## Anexo: `imshow(title, img, bgr=True)`\n",
    "Una utilidad habitual para evitar errores de conversión de color.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def imshow(title, img, bgr=True):\n",
    "    plt.figure()\n",
    "    if img.ndim == 2:\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    else:\n",
    "        if bgr:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Ejemplo en una MISMA celda: mostrar BGR y RGB lado a lado usando la función\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].imshow(img)  # BGR mal interpretado como RGB\n",
    "axes[0].set_title('BGR interpretado como RGB'); axes[0].axis('off')\n",
    "axes[1].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title('RGB correcto'); axes[1].axis('off')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
